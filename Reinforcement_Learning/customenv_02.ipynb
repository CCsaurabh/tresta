{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMC5rIgPmTeIW5rbC4TFP9V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### The Environment has been defined so as to train a model to move the player to the top-left using Gym Environment Formulation."
      ],
      "metadata": {
        "id": "gWAeZ19aFNRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![game.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "-sfmFHzFHzyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J57WjAVUy9au"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym \n",
        "from gym import spaces\n",
        "from stable_baselines3.common.env_checker import check_env"
      ],
      "metadata": {
        "id": "6vQGalnhzJSh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gym environment skeleton\n",
        "class Goleft_UpEnv(gym.Env):\n",
        "  metadata = {'render.modes':['console']}\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "  UP = 2 \n",
        "  DOWN = 3\n",
        "\n",
        "  def __init__(self,grid_size = 10):\n",
        "    super(Goleft_UpEnv,self).__init__()\n",
        "\n",
        "    #Size of 1d grid\n",
        "    self.grid_size = grid_size \n",
        "    #Initiate at the right bottom corner \n",
        "    self.agent_pos = [grid_size - 1,0]\n",
        "    n_actions = 4\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    self.observation_space = spaces.Box(low = np.array([0,0]).astype(np.float32), high=np.array([self.grid_size,self.grid_size]).astype(np.float32),dtype=np.float32)\n",
        "\n",
        "  def reset(self):\n",
        "    #Initialize agent at right bottom corner\n",
        "    self.agent_pos = [self.grid_size - 1, 0]\n",
        "    return np.array([self.agent_pos[0],self.agent_pos[1]]).astype(np.float32)\n",
        "\n",
        "  def step(self,action):\n",
        "    if (action==self.LEFT):\n",
        "      self.agent_pos[0] -=1\n",
        "    elif (action==self.RIGHT):\n",
        "      self.agent_pos[0] +=1\n",
        "    elif (action==self.UP):\n",
        "      self.agent_pos[1] +=1\n",
        "    elif (action==self.DOWN):\n",
        "      self.agent_pos[1] -=1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    #Account for boundaries of the grid\n",
        "    self.agent_pos[0] = np.clip(self.agent_pos[0],0,self.grid_size)\n",
        "    self.agent_pos[1] = np.clip(self.agent_pos[1],0,self.grid_size)\n",
        "\n",
        "    #Target place to reach = Top left\n",
        "    done = bool((self.agent_pos[0]==0)&(self.agent_pos[1]==self.grid_size-1))\n",
        "\n",
        "    if ((self.agent_pos[0]==0) & (self.agent_pos[1]== self.grid_size-1)):\n",
        "      reward = 1\n",
        "    else:\n",
        "      reward = 0\n",
        "\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.agent_pos[0],self.agent_pos[1]]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self,mode='console'):\n",
        "    if mode!='console':\n",
        "      raise NotImplementedError()\n",
        "    for i in range(self.grid_size):\n",
        "      if self.grid_size - 1 - self.agent_pos[1]!=i:\n",
        "        print(\".\" * (self.grid_size))\n",
        "      else:\n",
        "        print(\".\"*self.agent_pos[0], end=\"\")\n",
        "        print(\"x\",end=\"\")\n",
        "        print(\".\" * (self.grid_size - 1 - self.agent_pos[0]))\n",
        "  \n",
        "  def close(self):\n",
        "    pass"
      ],
      "metadata": {
        "id": "cXz0iYy-zMRY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Goleft_UpEnv(grid_size=4)\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "metadata": {
        "id": "YaghqTghzUZt"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the environment with random moves\n",
        "env = Goleft_UpEnv(grid_size=4)\n",
        "\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  move_direction = np.random.randint(4)\n",
        "  obs, reward, done, info = env.step(move_direction)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done, 'move_direction = ',move_direction)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4E860nKzUcU",
        "outputId": "28e5285d-f0a8-4ef3-be9e-c69814c442e1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "....\n",
            "....\n",
            "....\n",
            "...x\n",
            "Box([0. 0.], [4. 4.], (2,), float32)\n",
            "Discrete(4)\n",
            "1\n",
            "Step 1\n",
            "obs= [4. 0.] reward= 0 done= False move_direction =  1\n",
            "....\n",
            "....\n",
            "....\n",
            "....x\n",
            "Step 2\n",
            "obs= [3. 0.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            "....\n",
            "...x\n",
            "Step 3\n",
            "obs= [2. 0.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            "....\n",
            "..x.\n",
            "Step 4\n",
            "obs= [1. 0.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            "....\n",
            ".x..\n",
            "Step 5\n",
            "obs= [2. 0.] reward= 0 done= False move_direction =  1\n",
            "....\n",
            "....\n",
            "....\n",
            "..x.\n",
            "Step 6\n",
            "obs= [1. 0.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            "....\n",
            ".x..\n",
            "Step 7\n",
            "obs= [1. 0.] reward= 0 done= False move_direction =  3\n",
            "....\n",
            "....\n",
            "....\n",
            ".x..\n",
            "Step 8\n",
            "obs= [2. 0.] reward= 0 done= False move_direction =  1\n",
            "....\n",
            "....\n",
            "....\n",
            "..x.\n",
            "Step 9\n",
            "obs= [2. 0.] reward= 0 done= False move_direction =  3\n",
            "....\n",
            "....\n",
            "....\n",
            "..x.\n",
            "Step 10\n",
            "obs= [2. 1.] reward= 0 done= False move_direction =  2\n",
            "....\n",
            "....\n",
            "..x.\n",
            "....\n",
            "Step 11\n",
            "obs= [2. 0.] reward= 0 done= False move_direction =  3\n",
            "....\n",
            "....\n",
            "....\n",
            "..x.\n",
            "Step 12\n",
            "obs= [1. 0.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            "....\n",
            ".x..\n",
            "Step 13\n",
            "obs= [0. 0.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            "....\n",
            "x...\n",
            "Step 14\n",
            "obs= [1. 0.] reward= 0 done= False move_direction =  1\n",
            "....\n",
            "....\n",
            "....\n",
            ".x..\n",
            "Step 15\n",
            "obs= [0. 0.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            "....\n",
            "x...\n",
            "Step 16\n",
            "obs= [1. 0.] reward= 0 done= False move_direction =  1\n",
            "....\n",
            "....\n",
            "....\n",
            ".x..\n",
            "Step 17\n",
            "obs= [1. 1.] reward= 0 done= False move_direction =  2\n",
            "....\n",
            "....\n",
            ".x..\n",
            "....\n",
            "Step 18\n",
            "obs= [2. 1.] reward= 0 done= False move_direction =  1\n",
            "....\n",
            "....\n",
            "..x.\n",
            "....\n",
            "Step 19\n",
            "obs= [1. 1.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            ".x..\n",
            "....\n",
            "Step 20\n",
            "obs= [0. 1.] reward= 0 done= False move_direction =  0\n",
            "....\n",
            "....\n",
            "x...\n",
            "....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Try baselines\n",
        "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "env = Goleft_UpEnv(grid_size=4)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ],
      "metadata": {
        "id": "szbk8-qyzUfD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent\n",
        "model = A2C('MlpPolicy', env, verbose=1).learn(10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj7ZDCTTzUh0",
        "outputId": "f9d2ad06-1b88-42b4-e9ff-d990bb1a6a89"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 20.8     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 816      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.13    |\n",
            "|    explained_variance | -2.6     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | -0.136   |\n",
            "|    value_loss         | 0.00656  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 19.4     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 803      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.795   |\n",
            "|    explained_variance | -3.17    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -0.00166 |\n",
            "|    value_loss         | 0.000787 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 17.9     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 810      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.05    |\n",
            "|    explained_variance | -0.636   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 0.0134   |\n",
            "|    value_loss         | 0.00033  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 15.4     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 806      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.916   |\n",
            "|    explained_variance | -0.185   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 0.108    |\n",
            "|    value_loss         | 0.00562  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 12       |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 803      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.29    |\n",
            "|    explained_variance | -10.6    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -0.0695  |\n",
            "|    value_loss         | 0.00333  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 10.4     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 804      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.822   |\n",
            "|    explained_variance | -4.18    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 0.00191  |\n",
            "|    value_loss         | 0.00106  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 9.52     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 806      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.05    |\n",
            "|    explained_variance | -2.68    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 0.036    |\n",
            "|    value_loss         | 0.00107  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 9.34     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 805      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.995   |\n",
            "|    explained_variance | -1.05    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | 0.0603   |\n",
            "|    value_loss         | 0.0039   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 9.94     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 805      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.948   |\n",
            "|    explained_variance | -3.22    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -0.0326  |\n",
            "|    value_loss         | 0.000986 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 9.07     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 805      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.862   |\n",
            "|    explained_variance | 0.877    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 0.00322  |\n",
            "|    value_loss         | 9.35e-05 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 7.78     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 803      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.777   |\n",
            "|    explained_variance | -1.95    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 0.00613  |\n",
            "|    value_loss         | 0.00027  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 8.52     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 804      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.72    |\n",
            "|    explained_variance | -0.619   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -0.00475 |\n",
            "|    value_loss         | 0.000385 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 8.54     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 804      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.683   |\n",
            "|    explained_variance | -1.45    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -0.0442  |\n",
            "|    value_loss         | 0.00349  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 7.82     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 805      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.711   |\n",
            "|    explained_variance | -4.23    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -0.00307 |\n",
            "|    value_loss         | 0.00031  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 7.54     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 805      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.736   |\n",
            "|    explained_variance | -1.26    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -0.0113  |\n",
            "|    value_loss         | 0.000525 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 7.25     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 806      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.73    |\n",
            "|    explained_variance | 0.907    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -0.00314 |\n",
            "|    value_loss         | 5.44e-05 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 7.59     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 806      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.775   |\n",
            "|    explained_variance | -0.106   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -0.0133  |\n",
            "|    value_loss         | 0.000276 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 8.11     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 806      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.621   |\n",
            "|    explained_variance | 0.423    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 0.0156   |\n",
            "|    value_loss         | 0.000901 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 7.3      |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 807      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.643   |\n",
            "|    explained_variance | -2.28    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -0.024   |\n",
            "|    value_loss         | 0.00222  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 7.25     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 807      |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.667   |\n",
            "|    explained_variance | 0.771    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 0.0085   |\n",
            "|    value_loss         | 0.000431 |\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy34mdVXzdXi",
        "outputId": "69904f87-b086-4301-da38-3a542cd30e81"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  2\n",
            "obs= [3. 1.] reward= 0 done= False\n",
            "....\n",
            "....\n",
            "...x\n",
            "....\n",
            "Step 2\n",
            "Action:  0\n",
            "obs= [2. 1.] reward= 0 done= False\n",
            "....\n",
            "....\n",
            "..x.\n",
            "....\n",
            "Step 3\n",
            "Action:  0\n",
            "obs= [1. 1.] reward= 0 done= False\n",
            "....\n",
            "....\n",
            ".x..\n",
            "....\n",
            "Step 4\n",
            "Action:  2\n",
            "obs= [1. 2.] reward= 0 done= False\n",
            "....\n",
            ".x..\n",
            "....\n",
            "....\n",
            "Step 5\n",
            "Action:  0\n",
            "obs= [0. 2.] reward= 0 done= False\n",
            "....\n",
            "x...\n",
            "....\n",
            "....\n",
            "Step 6\n",
            "Action:  2\n",
            "obs= [0. 3.] reward= 1 done= True\n",
            "x...\n",
            "....\n",
            "....\n",
            "....\n",
            "Goal reached! reward= 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our player reached the end goal within 6 steps."
      ],
      "metadata": {
        "id": "YIl5gwQJFj7O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJLvOJ2Qzdcr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}